{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "try: \n",
    "    type(sc)\n",
    "except NameError:\n",
    "    sc = pyspark.SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "1) Contamos con un cluster que tiene 4 computadoras. Queremos aprovechar el paralelismo del cluster para calcular los números primos entre 2 y 20.000.000. Para esto usaremos el conocido algoritmo de la criba de Eratóstenes. Por ejemplo si empezamos con una lista de tipo (2,3,4,5,6,7,8...) en un primer paso eliminamos todos los que son mayores a 2 y divisibles por 2 y nos queda (2,3,5,7,9,11,13...) luego eliminamos todos los mayores a 3 divisibles por 3 y nos queda (2,3,5,7,11,13....etc) luego todos los divisibles por 5 y así sucesivamente. El resultado final es una lista de números que son primos. Programar este programa usando PySpark (**) (15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "2\n",
      "[2, 3, 5, 7, 9, 11, 13, 15, 17, 19]\n",
      "[2, 3, 5, 7, 9, 11, 13, 15, 17, 19]\n",
      "3\n",
      "[2, 3, 5, 7, 11, 13, 17, 19]\n",
      "[2, 3, 5, 7, 11, 13, 17, 19]\n",
      "5\n",
      "[2, 3, 5, 7, 11, 13, 17, 19]\n",
      "[2, 3, 5, 7, 11, 13, 17, 19]\n",
      "7\n",
      "[2, 3, 5, 7, 11, 13, 17, 19]\n",
      "[2, 3, 5, 7, 11, 13, 17, 19]\n",
      "11\n",
      "[2, 3, 5, 7, 11, 13, 17, 19]\n",
      "[2, 3, 5, 7, 11, 13, 17, 19]\n",
      "13\n",
      "[2, 3, 5, 7, 11, 13, 17, 19]\n",
      "[2, 3, 5, 7, 11, 13, 17, 19]\n",
      "17\n",
      "[2, 3, 5, 7, 11, 13, 17, 19]\n",
      "[2, 3, 5, 7, 11, 13, 17, 19]\n",
      "19\n",
      "[2, 3, 5, 7, 11, 13, 17, 19]\n"
     ]
    }
   ],
   "source": [
    "l= list(range(2, 20))\n",
    "rdd = sc.parallelize(l,4)\n",
    "pivote = 2\n",
    "nuevoPivote = 2\n",
    "continuar = True\n",
    "\n",
    "\n",
    "while(continuar):\n",
    "    try : \n",
    "        nuevoPivote = rdd.filter(lambda x: x > pivote).reduce(lambda x,y : x if x<y else y)\n",
    "        #Esto lo meto aca porque sino en algun momento va a querer hacer reduce de una lista vacia\n",
    "        continuar = True\n",
    "    except:\n",
    "        continuar = False\n",
    "    print rdd.collect()\n",
    "    print pivote\n",
    "    rdd = sc.parallelize(rdd.filter(lambda x:((x<=pivote)or(0 != (x%pivote)))).collect())\n",
    "    #hago esta mierda porque rdd = rdd.filter hacia lo que queria\n",
    "    print rdd.collect()\n",
    "    pivote = nuevoPivote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    " Se tiene un archivo que representa una red social con\n",
    "billones de links entre usuarios de la forma (id1,id2). Se\n",
    "quieren encontrar todos los triángulos de tipo (id1,id2,id3)\n",
    "tales que existen (id1,id2),(id2,id3),(id1,id3). Explicar, sin\n",
    "programar, que algoritmo usaría para listar todos los triángulos\n",
    "que existen en la red social usando Map Reduce. En concreto\n",
    "se pide para el siguiente fragmento del archivo:\n",
    "(A,B) (B,C) (A,C) (A,D) (C,D) (D,E) (D,F) (E,F) (A,E)\n",
    "- Qué recibe el metodo Map, qué hace y qué emite.\n",
    "- Qué recibe el metodo Reduce, qué hace y qué emite.\n",
    "Si lo hace en dos o mas iteraciones entonces explique los\n",
    "puntos anteriores por cada iteración. (****) (15pts)\n",
    "(opcional x puntos extras) ¿De qué forma podría usarse el\n",
    "contar la cantidad de triángulos para saber si un determinado\n",
    "grafo es o no una red social? Justifique. (*****) (5 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "l = [(1,2),(1,3),(2,3),(2,4),(3,4),(3,5),(4,5),(5,6)]\n",
    "print l\n",
    "\n",
    "rdd = sc.parallelize(l)\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def f(l):\n",
    "    comb = []\n",
    "    for i in range(0, len(l[1])):\n",
    "        for j in range(i+1, len(l[1])):\n",
    "                comb += [sorted([l[0],l[1][i],l[1][j]])]\n",
    "    return comb\n",
    "\n",
    "\n",
    "print rdd.map(lambda x: [(x[0],x[1]),(x[1],x[0])]).flatMap(lambda x : x)\\\n",
    ".map(lambda x : (x[0],[x[1]])).reduceByKey(lambda x,y : x+y).map(lambda x: f(x)).flatMap(lambda x:x).map(lambda x: ((x[0],x[1],x[2]),1)).reduceByKey(lambda x,y : x+y).filter(lambda x: x[1] >2).collect()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    }
   ],
   "source": [
    "l = sc.parallelize([1,2,3,4,23])\n",
    "print l.reduce(lambda x,y:x if x>y else y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Se tiene un RDD con las coordenadas de rectángulos de la forma \n",
    "(x1,x2,y1,y2). Se pide programar en PySpark un programa que\n",
    "encuentre el rectángulo de superficie mínima que contiene al punto(w,z) (*?) (15 pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 5, 4, 8), (3, 10, 3, 7), (1, 7, 1, 6), (4.5, 6, 3.4, 5), (2.5, 4, 3.2, 4.1), (2, 3, 2, 3.1)]\n",
      "(3, 3.4)\n",
      "((2.5, 4, 3.2, 4.1), 1.3499999999999992)\n"
     ]
    }
   ],
   "source": [
    "rect = sc.parallelize([(2,5,4,8),(3,10,3,7),(1,7,1,6),(4.5,6,3.4,5),(2.5,4,3.2,4.1),(2,3,2,3.1)])\n",
    "print rect.collect()\n",
    "p = (3,3.4)\n",
    "print p\n",
    "\n",
    "def f(x,p):\n",
    "    return ((p[0]>=x[0] and p[0]<=x[1] and p[1] >= x[2] and p[1] <= x[3]))\n",
    "\n",
    "def area(x):\n",
    "    return (abs(x[0]-x[1]) * abs(x[2]-x[3]))\n",
    "\n",
    "print rect.filter(lambda x: f(x,p)).map(lambda x: (x,area(x))).reduce(lambda x,y: x if x[1] < y[1] else y)\n",
    "# .collect()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "1) Dada una colección de documentos queremos encontrar\n",
    "frases de 1 , 2 o 3 palabras que sean anagramas de otras. Por\n",
    "ejemplo: (“Postmaster”, “Stamp store”) o (“A telescope” , “To\n",
    "see Place”) o (“The cockroach”, “cook catch her”). Esta tarea\n",
    "implica una combinatoria muy difícil por lo que se decide usar\n",
    "Map-Reduce para paralelizarla. Usando Map-Reduce programar\n",
    "la solución a este problema listando todos los pares de\n",
    "anagramas entre frases de 1, 2 o 3 palabras. Como puede\n",
    "verse en los ejemplos se ignoran mayúsculas y minúsculas y\n",
    "los espacios en blanco, puntuación, etc. Suponer que existe la\n",
    "función word_tokenizer que recibe un texto y devuelve un\n",
    "vector de palabras ya convertidas a minúsculas y sin\n",
    "puntuación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['postmaster see cook', 'stamp store a telescope', 'to see place cook catch her', 'the cockroach die with you']\n",
      "[('aemoprsstt', ['postmaster', 'stamp store']), ('acccehhkoort', ['cook catch her', 'the cockroach']), ('aceeelopst', ['a telescope', 'to see place'])]\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize(['postmaster see cook',\n",
    "                      'stamp store a telescope',\n",
    "                      'to see place cook catch her',\n",
    "                      'the cockroach die with you'])\n",
    "\n",
    "def frases(x):\n",
    "    w = []\n",
    "    x = x.split()\n",
    "    for i in range(0, len(x)):\n",
    "        w += [[x[i]]]\n",
    "    size = len(x)\n",
    "    for i in range(0,size-1):\n",
    "        w += [[x[i]+' '+ x[i+1]]]\n",
    "    for i in range(0, size-2):\n",
    "        w += [[x[i]+' '+x[i+1]+' '+x[i+2]]]\n",
    "    \n",
    "    return w\n",
    "\n",
    "def sortWord(s):\n",
    "    s = sorted(s[0])\n",
    "    l = []\n",
    "    for i in range(0,len(s)):\n",
    "        if s[i] != ' ':\n",
    "            l += s[i]\n",
    "    s = ''\n",
    "    for i in range(0,len(l)):\n",
    "        s += l[i]\n",
    "    return s\n",
    "    \n",
    "print rdd.map(lambda x: frases(x)).flatMap(lambda x: x).map(lambda x: (sortWord(x),[x[0]])).reduceByKey(lambda x,y :x+y).filter(lambda x :(len(x[1]) > 1) and (len(set(x[1])) >=len(x[1]))).collect()\n",
    "# .collect()\n",
    "# .map(lambda x: (x[0],).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "1) Se tiene un archivo con información sobre visitas a páginas\n",
    "web de la forma: (URL, visitas, fecha). Existe solo un registro\n",
    "por día para cada URL. Se quiere generar un archivo que, por\n",
    "cada URL, indique cuál fue la fecha en la que tuvo mas visitas\n",
    "y la cantidad de visitas.\n",
    "Programar lo pedido en Map Reduce usando agregación para\n",
    "minimizar la cantidad de datos que deben transferirse en la\n",
    "red.\n",
    "Atención: La resolución es muy simple, trivial, asi que es\n",
    "fundamental resolver la agregación para el puntaje completo.\n",
    "(**) (15 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('google', '23-2'), 2232526)\n"
     ]
    }
   ],
   "source": [
    "# (URL,vistas, fecha)\n",
    "\n",
    "rdd = sc.parallelize([\n",
    "    ('google',213,'23-2'),\n",
    "    ('google',2232313,'23-2'),\n",
    "    ('google',213,'22-2'),\n",
    "    ('google',213,'21-2')\n",
    "    ])\n",
    "print rdd.map(lambda x: ((x[0],x[2]),x[1])).reduceByKey(lambda x,y : x+y).reduce(lambda x,y: x if x[1]>y[1] else y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('google', '23-2'), 2232526), (('google', '22-2'), 213), (('google', '21-2'), 213)]\n"
     ]
    }
   ],
   "source": [
    "# Solucion usando aggregate\n",
    "\n",
    "print rdd.map(lambda x: ((x[0],x[2]),x[1])).aggregateByKey(0, lambda x,y: x + y , lambda x,y : x+y ).takeOrdered(3,lambda x: -x[1])\n",
    "# reduce(lambda x,y: x if x>y else y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Se tiene un grafo dirigido representado mediante una lista\n",
    "de adyacencias. Dado un nodo inicial y un nodo final queremos\n",
    "que programe usando Map-Reduce un algoritmo que indique\n",
    "cuál es el camino mínimo entre ambos nodos. En clase vimos\n",
    "como calcular la longitud, ahora queremos saber cuál es el\n",
    "camino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, [4, 5])\n",
      "[(4, [3, 5]), (1, [2, 3, 4]), (2, [4, 5]), (3, [5])]\n",
      "[[3, 5]]\n",
      "[[2, 4], [2, 5]]\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([(1,2),(1,3),(1,4),(2,4),(2,5),(3,5),(4,3),(4,5)])\n",
    "# rdd.collect()\n",
    "\n",
    "rdd = rdd.map(lambda x: (x[0],[x[1]])).reduceByKey(lambda x,y: x+y)\n",
    "# rdd.collect()\n",
    "\n",
    "def f(r,x,y):\n",
    "    path = []\n",
    "    adjsX = r.filter(lambda a: a[0] == x ).collect()\n",
    "    adjsX = adjsX[0]\n",
    "    print adjsX\n",
    "    \n",
    "    adjsDeLosAdjsDeX = []\n",
    "    print r.collect()\n",
    "    for i in range(0, len(adjsX[1])):\n",
    "        path.append([x,adjsX[1][i]])\n",
    "        \n",
    "        l = r.filter(lambda a: a[0] == adjsX[1][i]).collect()\n",
    "        if (l):\n",
    "            adjsDeLosAdjsDeX.append(l[0][1])\n",
    "    print adjsDeLosAdjsDeX\n",
    "    print path\n",
    "    \n",
    "\n",
    "    for i in range(0, len(adjsDeLosAdjsDeX)):\n",
    "        path[i] += adjsDeLosAdjsDeX\n",
    "        \n",
    "\n",
    "f(rdd,2,5)\n",
    "\n",
    "# TERMINAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "En este ejercicio queremos programar un sistema que recomiende textos a usuarios en base a sus gustos sobre ciertos términos (palabras).\n",
    "Se cuenta con un RDD de textos de la forma (docId, texto) donde texto es un string de longitud variable.\n",
    "Además contamos con un RDD que indica qué términos le gustan o no a cada usuario de la forma (userId, término, score) por ejemplo (23, “calesita”, -2).\n",
    "Se pide programar en Spark un programa que calcule el score total de cada documento para cada usuario generando un RDD de la forma (userId, docId, score) en donde el score es simplemente la suma de los scores del usuario para los términos que aparecen en el documento.\n",
    "Puede haber términos en los documentos para los cuales no exista score de algunos usuarios, en estos casos simplemente los consideramos neutros (score=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rdd1 = sc.parallelize([(1,'Hoy es lunes 12'),\n",
    "        (2, 'tenemos entrega de taller'),\n",
    "        (3,'la entrega esta como el orto'),\n",
    "        (4,'lunes parcial datos')])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rdd2 = sc.parallelize([('id1',\"taller\",4),\n",
    "        ('id1','Hoy',12),\n",
    "        ('id1', 'lunes',-19),\n",
    "        ('id2', 'lunes', -30),\n",
    "        ('id3','datos',20),\n",
    "        ('id2','datos',8),\n",
    "        ('id3','lunes',-20)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
